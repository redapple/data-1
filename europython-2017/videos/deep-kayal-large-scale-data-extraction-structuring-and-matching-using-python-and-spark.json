{
  "description": "Motivation - Matching data collections with the aim to augment and\nintegrate the information for any available data point that lies in\ntwo or more of these collections, is a problem that nowadays arises\noften. Notable examples of such data points are scientific\npublications for which metadata and data are kept in various\nrepositories, and users\u2019 profiles, whose metadata and data exist in\nseveral social networks or platforms.\n\nIn our case, collections were as follows: (1) A large dump of\ncompressed data files on s3 containing archives in the form of zips,\ntars, bzips and gzips, which were expected to contain published\npapers in the form of xmls and pdfs, amongst other files, and (2) A\nlarge store of xmls in the form of xmls, some of which are to be\nmatched to Collection 1.\n\nProblem Statement - The problems, then, are: (1) How to best unzip\nthe compressed archives and extract the relevant files? (2) How to\nextract meta-information from the xml or pdf files? (3) How to match\nthe meta-information from the two different collections? And all of\nthese must be done in a big-data environment.\n\nPresentation \u2013\nhttps://drive.google.com/open?id=1hA9J80446Qh7nd8PMYZibtIR1WjMkdLXfDgwUlts7JM\n\nThe presentation will describe the solution process and the use of\npython and Spark in the large-scale unzipping and extraction of files\nfrom archives, and how metadata was then extracted from the files to\nperform the matches on.",
  "duration": 1832,
  "recorded": "2017-07-14",
  "speakers": [
    "Deep Kayal"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/eV23M0Jtsbw/hqdefault.jpg",
  "title": "Large-scale data extraction, structuring and matching using Python and Spark",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=eV23M0Jtsbw"
    }
  ]
}
