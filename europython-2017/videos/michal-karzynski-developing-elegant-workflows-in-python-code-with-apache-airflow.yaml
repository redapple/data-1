description: |-
  Every time a new batch of data comes in, you start a set of tasks.
  Some tasks can run in parallel, some must run in a sequence, perhaps
  on a number of different machines. That's a workflow.

  Did you ever draw a block diagram of your workflow? Imagine you could
  bring that diagram to life and actually run it as it looks on the
  whiteboard. With Airflow you can just about do that.

  http://airflow.apache.org

  Apache Airflow is an open-source Python tool for orchestrating data
  processing pipelines. In each workflow tasks are arranged into a
  directed acyclic graph (DAG). Shape of this graph decides the overall
  logic of the workflow. A DAG can have many branches and you can
  decide which of them to follow and which to skip at execution time.

  This creates a resilient design because each task can be retried
  multiple times if an error occurs. Airflow can even be stopped
  entirely and running workflows will resume by restarting the last
  unfinished task. Logs for each task are stored separately and are
  easily accessible through a friendly web UI.

  In my talk I will go over basic Airflow concepts and through examples
  demonstrate how easy it is to define your own workflows in Python
  code. We'll also go over ways to extend Airflow by adding custom task
  operators, sensors and plugins.

duration: 1767
recorded: '2017-07-13'
speakers:
- Michał Karzyński
thumbnail_url: https://i.ytimg.com/vi/XJf-f56JbFM/hqdefault.jpg
title: Developing elegant workflows in Python code with Apache Airflow
videos:
- type: youtube
  url: https://www.youtube.com/watch?v=XJf-f56JbFM

